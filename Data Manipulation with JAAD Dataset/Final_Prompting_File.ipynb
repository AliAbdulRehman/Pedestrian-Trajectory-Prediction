{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfcf4d8",
   "metadata": {},
   "source": [
    "For Pedestrians with Behavior attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6ee12da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts generated and saved to respective folders.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "images_dir = \"./images\"\n",
    "output_main_dir = \"./Prompts_for_Pedestrians\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 1\n",
    "videos_processed = 0\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "        \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    ped_annotations = video_data['ped_annotations']        \n",
    "    num_frames = video_data['num_frames']\n",
    "    \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "        \n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown') \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in ped_annotations.items():\n",
    "        output_pedestrian_dir = output_video_dir  # Remove subfolder creation\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "        \n",
    "        all_video_prompts = []\n",
    "\n",
    "        for frame_num in pedestrian_data['frames']:\n",
    "            if frame_num <= num_frames:\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    \n",
    "                    age = age_mapping.get(pedestrian_data['attributes'].get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(pedestrian_data['attributes'].get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(pedestrian_data['attributes'].get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "                    if frame_num < len(pedestrian_data['bbox']):\n",
    "                        current_bbox = pedestrian_data['bbox'][frame_num]\n",
    "                    else:\n",
    "                        continue\n",
    "                             \n",
    "                    previous_bboxes = []\n",
    "                    for i in range(frame_index - 1, max(frame_index - 6, -1), -1):\n",
    "                        if pedestrian_data['frames'][i] < len(pedestrian_data['bbox']):\n",
    "                            previous_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "\n",
    "                    future_bboxes = []\n",
    "                    for i in range(frame_index + 1, min(frame_index + 6, len(pedestrian_data['frames']))):\n",
    "                        if i < len(pedestrian_data['frames']) and pedestrian_data['frames'][i] < len(pedestrian_data['bbox']):\n",
    "                            future_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "                        else:\n",
    "                            break  \n",
    "                    \n",
    "                    prompt = {\n",
    "                        \"id\": pedestrian_id,\n",
    "                        \"image\": f\"./images_with_boxes_Pedestrians Focused_/{video_id}/Pedestrian_{pedestrian_id}/Pedestrian_{pedestrian_id}_Image_{frame_num}.png\",\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"<image> Can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present and the past 5 frames? Is the pedestrian currently engaged in crossing the road? What is the pedestrian's motion direction? Can you tell the pedestrian's action? What is the pedestrian's reaction? Is the pedestrian making any specific hand gestures? Is the pedestrian looking at the vehicle? Is the pedestrian nodding? Also, can you tell me about the vehicle movement? Moreover can you describe the entire traffic scene? Can you also assist me in predicting the trajectories of pedestrians in the next 5 frames?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {pedestrian_data['bbox'][frame_index]}. The 5 bounding boxes for previous frames are: {previous_bboxes}. The pedestrian is {cross} the road. The pedestrian's motion direction is {motion_direction}. The pedestrian is currently {action}. The pedestrian's reaction is {reaction}. The pedestrian maintains a {hand_gesture} hand gesture throughout the video. The pedestrian is {look} at the vehicle and the pedestrian is {nod}. For the vehicle attributes, it is '{vehicle}'. While, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'. The future 5 bounding boxes are: {future_bboxes}.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                    \n",
    "        if all_video_prompts:\n",
    "\n",
    "            with open(os.path.join(output_video_dir, f'pedestrian_{pedestrian_id}_prompts.json'), 'w') as f:\n",
    "                json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "print(\"Prompts generated and saved to respective folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be2d7e",
   "metadata": {},
   "source": [
    "For Pedestrians without Behavior Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fcb62f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified prompts generated and saved to respective folders.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                    prompt = {\n",
    "                        \"id\": pedestrian_counter,\n",
    "                        \"image\": f\"./images_with_boxes_Pedestrians Focused_/{video_id}/Pedestrian_{pedestrian_id}/Pedestrian_{pedestrian_id}_Image_{frame_num}.png\",\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"<image> Can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present and the past 5 frames? Is the pedestrian currently engaged in crossing the road? What is the pedestrian's motion direction? Can you tell the pedestrian's action? What is the pedestrian's reaction? Is the pedestrian making any specific hand gestures? Is the pedestrian looking at the vehicle? Is the pedestrian nodding? Also, can you tell me about the vehicle movement? Moreover can you describe the entire traffic scene? Can you also assist me in predicting the trajectories of pedestrians in the next 5 frames?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {pedestrian_data['bbox'][frame_index]}. The 5 bounding boxes for previous frames are: {previous_bboxes}. The pedestrian is {cross} the road. The pedestrian's motion direction is {motion_direction}. The pedestrian is currently {action}. The pedestrian's reaction is {reaction}. The pedestrian maintains a {hand_gesture} hand gesture throughout the video. The pedestrian is {look} at the vehicle and the pedestrian is {nod}. For the vehicle attributes, it is '{vehicle}'. While, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'. The future 5 bounding boxes are: {future_bboxes}.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "images_dir = \"./images\"\n",
    "output_main_dir = \"./Prompts_for_Pedestrians\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 1\n",
    "videos_processed = 0\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "        \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    num_frames = video_data['num_frames']\n",
    "    \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "        \n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown') \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in ped_annotations.items():\n",
    "        output_pedestrian_dir = output_video_dir  # Remove subfolder creation\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "\n",
    "        simplified_prompts = []\n",
    "\n",
    "        if 'behavior' not in pedestrian_data or not pedestrian_data['behavior']:\n",
    "\n",
    "            for frame_num in pedestrian_data['frames']:\n",
    "                if frame_num <= video_data['num_frames']:\n",
    "                    frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                    if frame_index < len(pedestrian_data['bbox']):\n",
    "                        current_bbox = pedestrian_data['bbox'][frame_index]\n",
    "                    else:\n",
    "                        continue  \n",
    "\n",
    "                    previous_bboxes = []\n",
    "                    for i in range(max(frame_index - 5, 0), frame_index):\n",
    "                        if i < len(pedestrian_data['bbox']):\n",
    "                            previous_bboxes.append(pedestrian_data['bbox'][i])\n",
    "\n",
    "                    future_bboxes = []\n",
    "                    for i in range(frame_index + 1, min(frame_index + 6, len(pedestrian_data['bbox']))):\n",
    "                        future_bboxes.append(pedestrian_data['bbox'][i])\n",
    "\n",
    "                    simplified_prompt = {\n",
    "                        \"id\": pedestrian_id,\n",
    "                        \"image\": f\"./images_with_boxes_Pedestrians Focused_/{video_id}/Pedestrian_{pedestrian_id}/Pedestrian_{pedestrian_id}_Image_{frame_num}.png\",\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\": <image> Can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present and for 5 previous frames?  Also, can you tell me about the vehicle movement? Moreover can you describe the entire traffic scene? Can you also assist me in predicting the trajectories of pedestrians in the next 5 frames?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {current_bbox}. The 5 bounding boxes for previous frames are: {previous_bboxes}. For the vehicle attributes, it is '{vehicle}'. While, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'. The future 5 bounding boxes are: {future_bboxes}.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    simplified_prompts.append(simplified_prompt)  # Append the simplified prompt to the list\n",
    "\n",
    "            with open(os.path.join(output_pedestrian_dir, f'pedestrian_{pedestrian_id}_prompts.json'), 'w') as f:\n",
    "                json.dump(simplified_prompts, f, indent=4)\n",
    "\n",
    "            videos_processed += 1\n",
    "\n",
    "print(\"Simplified prompts generated and saved to respective folders.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
