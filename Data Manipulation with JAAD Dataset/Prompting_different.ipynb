{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46027d1",
   "metadata": {},
   "source": [
    "# Creation of Prompts for the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c672772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# Mapping of numerical values to words\n",
    "gender_mapping = {0: 'Unknown', 1: 'Male', 2: 'Female'}\n",
    "age_mapping = {0: 'Unknown', 1: 'Child', 2: 'Adult', 3: 'Elderly'}\n",
    "group_size_mapping = {0: 'Unknown', 1: 'Alone', 2: 'Small Group', 3: 'Large Group'}\n",
    "motion_direction_mapping = {0: 'Unknown', 1: 'Forward', 2: 'Backward', 3: 'Left', 4: 'Right'}\n",
    "vehicle_action_mapping = {0: 'Unknown', 1: 'Stopped', 2: 'Moving Forward', 3: 'Turning Left', 4: 'Turning Right'}\n",
    "presence_absence_mapping = {0: 'Absent', 1: 'Present'}\n",
    "road_type_mapping = {0: 'Unknown', 1: 'Parking Lot', 2: 'Urban Road', 3: 'Highway'}\n",
    "\n",
    "# Path to the .pkl file\n",
    "pkl_file_path = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "# Path to save the JSON file\n",
    "json_file_path = \"./jaad_prompts_final.json\"\n",
    "\n",
    "# Load the data from the .pkl file\n",
    "with open(pkl_file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# List to store prompts\n",
    "prompts = []\n",
    "\n",
    "# Iterate through the videos\n",
    "for vid, vid_data in data.items():\n",
    "    num_frames = vid_data['num_frames']\n",
    "    # Generate prompts for every 50th frame\n",
    "    for frame_num in range(0, num_frames, 50):\n",
    "        prompt = f\"1. We are working on {vid}.\\n\"\n",
    "        prompt += f\"2. We examine each pedestrian's behavior in frame {frame_num}. In this frame, pedestrians are {presence_absence_mapping[1]}.\\n\"\n",
    "        pedestrian_annotations = vid_data.get('ped_annotations', {})\n",
    "        for ped_id, ped_data in pedestrian_annotations.items():\n",
    "            if frame_num in ped_data.get('frames', []):\n",
    "                pedestrian_gender = gender_mapping.get(ped_data.get('attributes', {}).get('gender', 0))\n",
    "                pedestrian_age = age_mapping.get(ped_data.get('attributes', {}).get('age', 0))\n",
    "                pedestrian_group_size = group_size_mapping.get(ped_data.get('attributes', {}).get('group_size', 0))\n",
    "                pedestrian_motion_direction = motion_direction_mapping.get(ped_data.get('attributes', {}).get('motion_direction', 0))\n",
    "                pedestrian_bbox = ped_data.get('bbox', [])\n",
    "                if len(pedestrian_bbox) > frame_num:\n",
    "                    pedestrian_bbox_info = pedestrian_bbox[frame_num]\n",
    "                else:\n",
    "                    pedestrian_bbox_info = \"Not available\"\n",
    "                prompt += f\"    - Gender: {pedestrian_gender}\\n\"\n",
    "                prompt += f\"    - Age: {pedestrian_age}\\n\"\n",
    "                prompt += f\"    - Group Size: {pedestrian_group_size}\\n\"\n",
    "                prompt += f\"    - Motion Direction: {pedestrian_motion_direction}\\n\"\n",
    "                prompt += f\"    - Bounding Box: {pedestrian_bbox_info}\\n\"\n",
    "\n",
    "        vehicle_annotations = vid_data.get('vehicle_annotations', {})\n",
    "        vehicle_action = vehicle_annotations.get(frame_num, 'Unknown')\n",
    "        vehicle_action_word = vehicle_action_mapping.get(vehicle_action, 'Unknown')\n",
    "        if vehicle_action != 'Unknown':\n",
    "            prompt += f\"3. Next, we inspect vehicle actions observed in the video frames. In this frame, the vehicle is {vehicle_action_word}.\\n\"\n",
    "        else:\n",
    "            prompt += f\"3. There are no vehicle annotations available.\\n\"\n",
    "\n",
    "        # Explore traffic-related attributes for the current frame\n",
    "        traffic_annotations = vid_data.get('traffic_annotations', {}).get(frame_num, {})\n",
    "        road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "        pedestrian_crossings = presence_absence_mapping.get(traffic_annotations.get('ped_crossing', 0))\n",
    "        pedestrian_signs = presence_absence_mapping.get(traffic_annotations.get('ped_sign', 0))\n",
    "        stop_signs = presence_absence_mapping.get(traffic_annotations.get('stop_sign', 0))\n",
    "        traffic_lights = presence_absence_mapping.get(traffic_annotations.get('traffic_light', 0))\n",
    "        prompt += f\"4. Finally, we explore traffic-related attributes. In this frame, the road type is {road_type}. It {presence_absence_mapping[1]} the following attributes:\\n\"\n",
    "        prompt += f\"    - Pedestrian Crossings: {pedestrian_crossings}\\n\"\n",
    "        prompt += f\"    - Pedestrian Signs: {pedestrian_signs}\\n\"\n",
    "        prompt += f\"    - Stop Signs: {stop_signs}\\n\"\n",
    "        prompt += f\"    - Traffic Lights: {traffic_lights}\\n\"\n",
    "\n",
    "        # Append prompt to the list\n",
    "        prompts.append(prompt)\n",
    "\n",
    "# Write prompts to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json_file.write(\"\\n\".join(prompts))\n",
    "\n",
    "print(\"Prompts saved to\", json_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c067f",
   "metadata": {},
   "source": [
    "Prompts for the entire video, focusing only on the first pedestrian of each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99100453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# Mapping of numerical values to words\n",
    "gender_mapping = {0: 'Unknown', 1: 'Male', 2: 'Female'}\n",
    "age_mapping = {0: 'Unknown', 1: 'Child', 2: 'Adult', 3: 'Elderly'}\n",
    "group_size_mapping = {0: 'Unknown', 1: 'Alone', 2: 'Small Group', 3: 'Large Group'}\n",
    "motion_direction_mapping = {0: 'Unknown', 1: 'Forward', 2: 'Backward', 3: 'Left', 4: 'Right'}\n",
    "vehicle_action_mapping = {0: 'Unknown', 1: 'Stopped', 2: 'Moving Forward', 3: 'Turning Left', 4: 'Turning Right'}\n",
    "presence_absence_mapping = {0: 'Absent', 1: 'Present'}\n",
    "road_type_mapping = {0: 'Unknown', 1: 'Parking Lot', 2: 'Urban Road', 3: 'Highway'}\n",
    "\n",
    "# Path to the .pkl file\n",
    "pkl_file_path = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "# Path to save the JSON file\n",
    "json_file_path = \"./jaad_prompts_revised.json\"\n",
    "\n",
    "# Load the data from the .pkl file\n",
    "with open(pkl_file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# List to store prompts for all videos\n",
    "all_video_prompts = []\n",
    "\n",
    "# Iterate through each video\n",
    "for video_id, video_data in data.items():\n",
    "    # Get the number of frames in the video\n",
    "    num_frames = video_data['num_frames']\n",
    "    # Get the middle frame number\n",
    "    middle_frame_num = num_frames // 2\n",
    "    \n",
    "    # Get pedestrian data for the middle frame\n",
    "    pedestrian_annotations = video_data.get('ped_annotations', {})\n",
    "    pedestrians_data = []\n",
    "    pedestrian_added = False\n",
    "    for ped_id, ped_data in pedestrian_annotations.items():\n",
    "        if middle_frame_num in ped_data.get('frames', []):\n",
    "            if not pedestrian_added:\n",
    "                bbox_data = ped_data.get('bbox', [])\n",
    "                if len(bbox_data) > middle_frame_num:\n",
    "                    bounding_box = bbox_data[middle_frame_num]\n",
    "                else:\n",
    "                    bounding_box = \"Not available\"\n",
    "                pedestrians_data.append({\n",
    "                    \"Types of Pedestrians\": \"Alone\",  # Assuming group size is always alone in this example\n",
    "                    \"Motion Direction\": \"Backward\",  # Assuming motion direction is always backward in this example\n",
    "                    \"Bounding Box\": bounding_box\n",
    "                })\n",
    "                pedestrian_added = True\n",
    "\n",
    "    # Check if pedestrian data exists for the middle frame\n",
    "    if not pedestrians_data:\n",
    "        continue\n",
    "\n",
    "    # Get vehicle data for the middle frame\n",
    "    vehicle_annotations = video_data.get('vehicle_annotations', {})\n",
    "    vehicle_action = vehicle_annotations.get(middle_frame_num, 'Unknown')\n",
    "\n",
    "    # Get traffic data for the middle frame\n",
    "    traffic_annotations = video_data.get('traffic_annotations', {}).get(middle_frame_num, {})\n",
    "    road_type = \"Unknown\"\n",
    "    pedestrian_crossings = \"Absent\"\n",
    "    pedestrian_signs = \"Absent\"\n",
    "    stop_signs = \"Absent\"\n",
    "    traffic_lights = \"Absent\"\n",
    "\n",
    "    # Generate the prompt for the current video\n",
    "    prompt = f\"Role: You are tasked with enhancing the pedestrian detection system for an autonomous vehicle. Develop a strategy to improve the accuracy and efficiency of pedestrian detection while minimizing false positives.\\n\\n\"\n",
    "    prompt += f\"Context:\\n\"\n",
    "    prompt += f\"• Pedestrian Behavior: We are examining {video_id}. In this video, the pedestrians are Present and have the following attributes:\\n\"\n",
    "    prompt += f\"  - Types of Pedestrians: {pedestrians_data[0]['Types of Pedestrians']}\\n\"\n",
    "    prompt += f\"  - Motion Direction: {pedestrians_data[0]['Motion Direction']}\\n\"\n",
    "    prompt += f\"• Vehicle Actions: Now we examine the vehicle actions. In this video, the vehicle is {vehicle_action}.\\n\"\n",
    "    prompt += f\"• Traffic Attributes: Now we examine the traffic attributes. In this video, the road type is {road_type}, with the following attributes:\\n\"\n",
    "    prompt += f\"  - Road type: {road_type}\\n\"\n",
    "    prompt += f\"  - Pedestrian Crossings: {pedestrian_crossings}\\n\"\n",
    "    prompt += f\"  - Pedestrian Signs: {pedestrian_signs}\\n\"\n",
    "    prompt += f\"  - Stop Signs: {stop_signs}\\n\"\n",
    "    prompt += f\"  - Traffic Lights: {traffic_lights}\\n\\n\"\n",
    "    prompt += f\"Task:\\n\"\n",
    "    prompt += f\"• Analysis: Review pedestrian behavior and vehicle status in the entire video.\\n\"\n",
    "    prompt += f\"• Trajectory Prediction: Predict the trajectories of predictions in the next 3 seconds or 6 next bounding boxes.\\n\\n\"\n",
    "\n",
    "    # Append the prompt to the list of all video prompts\n",
    "    all_video_prompts.append(prompt)\n",
    "\n",
    "# Write prompts to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    for video_prompt in all_video_prompts:\n",
    "        json_file.write(video_prompt)\n",
    "\n",
    "print(\"Prompts saved to\", json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263eb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Path to annotations .pkl file\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "# Path to the directory containing images\n",
    "images_dir = \"./images\"\n",
    "\n",
    "# Output directory to store images with bounding boxes\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "# Initialize list to store all video prompts\n",
    "all_video_prompts = []\n",
    "\n",
    "# Load the database\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "# Process the first 10 videos\n",
    "num_videos_to_process = 10\n",
    "videos_processed = 0\n",
    "\n",
    "# Specify the maximum number of images per pedestrian\n",
    "max_images_per_pedestrian = 5\n",
    "\n",
    "# New mapping\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'undefined', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "    \n",
    "    # Create a directory for the current video\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    # Get the number of frames in the video\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    # Process each pedestrian in the video\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        # Extract pedestrian attributes for the middle frame\n",
    "        middle_frame_index = len(pedestrian_data['frames']) // 2\n",
    "        middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "        age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "        gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "        motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "        action = action_mapping.get(pedestrian_data.get('action', 0), 'Unknown')\n",
    "        cross = cross_mapping.get(pedestrian_data.get('cross', 0), 'Unknown')\n",
    "        reaction = reaction_mapping.get(pedestrian_data.get('reaction', 0), 'Unknown')\n",
    "        hand_gesture = hand_gesture_mapping.get(pedestrian_data.get('hand_gesture', 0), 'Unknown')\n",
    "        look = look_mapping.get(pedestrian_data.get('look', 0), 'Unknown')\n",
    "        nod = nod_mapping.get(pedestrian_data.get('nod', 0), 'Unknown')\n",
    "        vehicle = vehicle_mapping.get(pedestrian_data.get('vehicle', 0), 'Unknown')\n",
    "        road_type = road_type_mapping.get(pedestrian_data.get('road_type', 0), 'Unknown')\n",
    "        pedestrian_crossing = pedestrian_crossing_mapping.get(pedestrian_data.get('pedestrian_crossing', 0), 'Unknown')\n",
    "        pedestrian_sign = pedestrian_sign_mapping.get(pedestrian_data.get('pedestrian_sign', 0), 'Unknown')\n",
    "        stop_sign = stop_sign_mapping.get(pedestrian_data.get('stop_sign', 0), 'Unknown')\n",
    "        traffic_light = traffic_light_mapping.get(pedestrian_data.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "        # Create a directory for the current pedestrian inside the output_video_dir\n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize a counter for the number of images processed for the current pedestrian\n",
    "        images_processed = 0\n",
    "\n",
    "        # List to store image paths for the current pedestrian\n",
    "        image_paths = []\n",
    "\n",
    "        # Process the frames for this pedestrian\n",
    "        for frame_num in pedestrian_data['frames']:\n",
    "            # Check if the current frame is within the bounds of the video\n",
    "            if frame_num <= num_frames:\n",
    "                # Check if the frame number is divisible by 10 and the maximum number of images has not been reached\n",
    "                if frame_num % 10 == 1 and images_processed < max_images_per_pedestrian:\n",
    "                    # Load the image for the current frame\n",
    "                    image_file = f\"{frame_num:05d}.png\"\n",
    "                    image_path = os.path.join(images_dir, video_id, image_file)\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Image file {image_file} does not exist for video {video_id}. Skipping...\")\n",
    "                        continue\n",
    "\n",
    "                    # Add the image path to the list\n",
    "                    image_paths.append(image_path)\n",
    "\n",
    "                    # Increment the counter for images processed\n",
    "                    images_processed += 1\n",
    "\n",
    "                    # Check if the maximum number of images has been reached\n",
    "                    if images_processed >= max_images_per_pedestrian:\n",
    "                        break\n",
    "\n",
    "        # Pad the image_paths list with empty strings if needed\n",
    "        while len(image_paths) < max_images_per_pedestrian:\n",
    "            image_paths.append(\"\")\n",
    "\n",
    "        # Generate the prompt for the current pedestrian\n",
    "        prompt = {\n",
    "            \"Video ID\": video_id,\n",
    "            \"Ped_id\": pedestrian_id,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": f\"Role: You are an autonomous vehicle that uses front-camera images to interact with pedestrians. Input: {input_images_line}. Above are 5 sequential ego-vehicle front-camera view images extraced from a 2 second video that you can see behind the wheel.{bounding_box_info}\\nTask: predict the trajectory of the pedestrian of interest for the next 1 second.\\nExpected output: coordinates of 6 bounding box indicating the trajectory of pedestrain for the next 1 second(in the form of [((al1,bl1), (ar1,br1))],[((al6, b6), (ar6, br6))]).\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": \"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {} in {}, as well as the vehicle movement and the entire traffic scene?\".format(pedestrian_id, video_id)\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": \"Of course. The pedestrian identified as '{}' is of the age group: {}. The gender is: {}. Its motion direction is {}, and it is currently {}. The pedestrian is {} the road, while its reaction is {}. Also, The pedestrian maintains a {} hand gesture throughout. The pedestrian is {}. The pedestrian nodding state is {}. For the vehicle, it is '{}'. Moreover, for the scene attributes, the road type is '{}', the pedestrian crossing is '{}', pedestrian sign is '{}', stop sign is '{}', and the traffic light is '{}'.\".format(\n",
    "                        pedestrian_id, age, gender, motion_direction, action, cross, reaction, hand_gesture, look,  nod, vehicle, road_type, pedestrian_crossing, pedestrian_sign, stop_sign, traffic_light)\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": \"Sure, I can help with that. Let's analyze the pedestrian behavior and predict their trajectories for the next 0.5 and 1 seconds.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Append the prompt to the list of all video prompts\n",
    "        all_video_prompts.append(prompt)\n",
    "\n",
    "    # Increment the counter for videos processed\n",
    "    videos_processed += 1\n",
    "\n",
    "# Write prompts to a JSON file\n",
    "with open('output_prompts_updated.json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_updated.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Path to annotations .pkl file\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "# Path to the directory containing images\n",
    "images_dir = \"./images\"\n",
    "\n",
    "# Output directory to store images with bounding boxes\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "# Initialize list to store all video prompts\n",
    "all_video_prompts = []\n",
    "\n",
    "# Load the database\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "# Process the first 10 videos\n",
    "num_videos_to_process = 5\n",
    "videos_processed = 0\n",
    "\n",
    "# Specify the maximum number of images per pedestrian\n",
    "max_images_per_pedestrian = 5\n",
    "\n",
    "# Mapping of numerical values to words\n",
    "gender_mapping = {0: 'Unknown', 1: 'Male', 2: 'Female'}\n",
    "age_mapping = {0: 'Unknown', 1: 'Child', 2: 'Adult', 3: 'Elderly'}\n",
    "group_size_mapping = {0: 'Unknown', 1: 'Alone', 2: 'Small Group', 3: 'Large Group'}\n",
    "motion_direction_mapping = {0: 'Unknown', 1: 'Forward', 2: 'Backward', 3: 'Left', 4: 'Right'}\n",
    "vehicle_action_mapping = {0: 'Unknown', 1: 'Stopped', 2: 'Moving Forward', 3: 'Turning Left', 4: 'Turning Right'}\n",
    "presence_absence_mapping = {0: 'Absent', 1: 'Present'}\n",
    "road_type_mapping = {0: 'Unknown', 1: 'Parking Lot', 2: 'Urban Road', 3: 'Highway'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "    \n",
    "    # Create a directory for the current video\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    # Get the number of frames in the video\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    # Process each pedestrian in the video\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        # Extract pedestrian attributes for the middle frame\n",
    "        middle_frame_index = len(pedestrian_data['frames']) // 2\n",
    "        middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "        age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "        gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "        motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "\n",
    "        # Get vehicle action for middle frame index\n",
    "        vehicle_action = vehicle_action_mapping.get(video_data['vehicle_annotations'].get('frames', {}).get(middle_frame_index, {}).get('action', 0), 'Unknown')\n",
    "\n",
    "        # Get traffic annotations for middle frame index\n",
    "        traffic_annotations = video_data['traffic_annotations'].get(middle_frame_index, {})\n",
    "\n",
    "        road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "        ped_crossing = presence_absence_mapping.get(traffic_annotations.get('ped_crossing', 0), 'Unknown')\n",
    "        ped_sign = presence_absence_mapping.get(traffic_annotations.get('ped_sign', 0), 'Unknown')\n",
    "        stop_sign = presence_absence_mapping.get(traffic_annotations.get('stop_sign', 0), 'Unknown')\n",
    "        traffic_light = presence_absence_mapping.get(traffic_annotations.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "        # Create a directory for the current pedestrian\n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize a counter for the number of images processed for the current pedestrian\n",
    "        images_processed = 0\n",
    "\n",
    "        # List to store image paths for the current pedestrian\n",
    "        image_paths = []\n",
    "\n",
    "        # Process the frames for this pedestrian\n",
    "        for frame_num in pedestrian_data['frames']:\n",
    "            # Check if the current frame is within the bounds of the video\n",
    "            if frame_num <= num_frames:\n",
    "                # Check if the frame number is divisible by 10 and the maximum number of images has not been reached\n",
    "                if frame_num % 10 == 1 and images_processed < max_images_per_pedestrian:\n",
    "                    # Load the image for the current frame\n",
    "                    image_file = f\"{frame_num:05d}.png\"\n",
    "                    image_path = os.path.join(images_dir, video_id, image_file)\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Image file {image_file} does not exist for video {video_id}. Skipping...\")\n",
    "                        continue\n",
    "\n",
    "                    # Add the image path to the list\n",
    "                    image_paths.append(image_path)\n",
    "\n",
    "                    # Increment the counter for images processed\n",
    "                    images_processed += 1\n",
    "\n",
    "                    # Check if the maximum number of images has been reached\n",
    "                    if images_processed >= max_images_per_pedestrian:\n",
    "                        break\n",
    "\n",
    "        # Generate the prompt for the current pedestrian\n",
    "        prompt = {\n",
    "            \"Video ID\": video_id,\n",
    "            \"Ped_id\": pedestrian_id,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": \" Picture 1: <img>{}</img>\\nPicture 2: <img>{}</img>\\nPicture 3: <img>{}</img>\\nPicture 4: <img>{}</img>\\nPicture 5: <img>{}</img>\".format(*image_paths)\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": \"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {} in {}, as well as the vehicle movement and the entire traffic scene?\".format(pedestrian_id, video_id)\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": \"Of course. The pedestrian identified as '{}' exhibits the following characteristics:\\n- Age: {}\\n- Gender: {}\\n- Motion Direction: {}\\nThe pedestrian does not cross the road in any frame.\\nThere is no reaction observed from the pedestrian in any frame.\\nThe pedestrian maintains a neutral hand gesture throughout.\\nThe pedestrian is not looking in any frame.\\nNo action is taken by the pedestrian in any frame.\\nThere is no nodding observed from the pedestrian in any frame.\\nFor the vehicle, it is 'moving forward'. Moreover, for the scene attributes, the road type is 'unknown', and there are 'a/no' pedestrian crossings, 'a/no' pedestrian signs, 'a/no' stop signs, or 'a/no' traffic lights detected.\".format(pedestrian_id, age, gender, motion_direction)\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": \"Sure, I can help with that. Let's analyze the pedestrian behavior and predict their trajectories for the next 0.5 and 1 seconds.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Append the prompt to the list of all video prompts\n",
    "        all_video_prompts.append(prompt)\n",
    "\n",
    "    # Increment the counter for videos processed\n",
    "    videos_processed += 1\n",
    "\n",
    "# Write prompts to a JSON file\n",
    "with open('output_prompts_old.json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_old.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b13d0",
   "metadata": {},
   "source": [
    "5 Prompts skipped by an interval of 10 frames, for every pedestrian in every video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "images_dir = \"./images\"\n",
    "\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 1\n",
    "videos_processed = 0\n",
    "\n",
    "max_frames_per_pedestrian = 5\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'undefined', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "all_video_prompts = []\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    #print(output_pedestrian_dir)\n",
    "    #os.makedirs(output_video_dir, exist_ok=True)\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    #print(vehicle_annotations.keys())\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    #print(traffic_annotations.keys())\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    #print(ped_annotations.keys())\n",
    "        \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "    '''\n",
    "    print(\"Keys and Values in first_frame_traffic_attributes:\")\n",
    "    for key, value in first_frame_traffic_attributes.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    '''\n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        \n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "\n",
    "        middle_frame_index = len(pedestrian_data['frames']) // 2\n",
    "        middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "        frames_processed = 0\n",
    "\n",
    "        first_frame_number = pedestrian_data['frames'][0]\n",
    "\n",
    "        frames_to_process = pedestrian_data['frames'][::10][:max_frames_per_pedestrian]\n",
    "        for frame_num in frames_to_process:\n",
    "            if frame_num <= num_frames:\n",
    "                frames_processed += 1\n",
    "\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "\n",
    "                    bounding_box_info = \"\"\n",
    "                    if frame_num in pedestrian_data['bbox']:\n",
    "                        bbox = pedestrian_data['bbox'][frame_num]\n",
    "                        bounding_box_info = f\"in the bounding box: {bbox}, \"\n",
    "                        \n",
    "                    next_bounding_box_coordinates = []\n",
    "                    for i in range(1, 3):\n",
    "                        next_frame_index = frame_index + i * 10\n",
    "                        if next_frame_index < len(pedestrian_data['frames']):\n",
    "                            next_bbox = pedestrian_data['bbox'][next_frame_index]\n",
    "                            next_bounding_box_coordinates.append(next_bbox)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    next_bounding_box_info = \"\"\n",
    "                    if next_bounding_box_coordinates:\n",
    "                        next_bounding_box_info = \"The predicted trajectory for the pedestrian for the next 1 second is: \"\n",
    "                        next_bounding_box_info += ', '.join([f\"[{bbox[0]}, {bbox[1]}]\" for bbox in next_bounding_box_coordinates if bbox])\n",
    "                        next_bounding_box_info += \" respectively.\"\n",
    "                    \n",
    "                    \n",
    "                    prompt = {\n",
    "                        \"Video ID\": video_id,\n",
    "                        \"Ped_id\": pedestrian_id,\n",
    "                        \"Frame Number\": frame_num,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\" Picture: {os.path.join(output_pedestrian_dir, f'Pedestrian_{pedestrian_id}_Image_{frame_num}.png')}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {pedestrian_id} in {video_id}, as well as the vehicle movement and the entire traffic scene?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Of course. The pedestrian identified as '{pedestrian_id}' is present in the bounding box: {pedestrian_data['bbox'][frame_index]}, is of the age group: {age}, and has gender is: {gender}. Its motion direction is {motion_direction}, and it is currently {action}. The pedestrian is {cross} the road, while its reaction is {reaction}. Also, The pedestrian maintains a {hand_gesture} hand gesture throughout. The pedestrian is {look}. The pedestrian nodding state is {nod}. For the vehicle, it is '{vehicle}'. Moreover, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": \"Sure, I can help with that. Let's analyze the pedestrian behavior and predict their trajectories for the next 0.5 and 1 seconds.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                    \n",
    "                    #print(\"Prompt:\")\n",
    "                    #print(json.dumps(prompt, indent=4))\n",
    "                    \n",
    "                if frames_processed >= max_frames_per_pedestrian:\n",
    "                    break\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "with open('output_prompts_Kiran(Original).json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_Kiran(Original).json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b47521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "images_dir = \"./images\"\n",
    "\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 346\n",
    "videos_processed = 0\n",
    "\n",
    "max_frames_per_pedestrian = 5\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "all_video_prompts = []\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    #print(output_pedestrian_dir)\n",
    "    #os.makedirs(output_video_dir, exist_ok=True)\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    #print(vehicle_annotations.keys())\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    #print(traffic_annotations.keys())\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    #print(ped_annotations.keys())\n",
    "        \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "    '''\n",
    "    print(\"Keys and Values in first_frame_traffic_attributes:\")\n",
    "    for key, value in first_frame_traffic_attributes.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    '''\n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        \n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "\n",
    "        middle_frame_index = len(pedestrian_data['frames']) // 2\n",
    "        middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "        frames_processed = 0\n",
    "\n",
    "        first_frame_number = pedestrian_data['frames'][0]\n",
    "\n",
    "        frames_to_process = pedestrian_data['frames'][::10][:max_frames_per_pedestrian]\n",
    "        \n",
    "        for frame_num in frames_to_process:\n",
    "            if frame_num <= num_frames:\n",
    "                frames_processed += 1\n",
    "\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "\n",
    "                    bounding_box_info = \"\"\n",
    "                    if frame_num in pedestrian_data['bbox']:\n",
    "                        bbox = pedestrian_data['bbox'][frame_num]\n",
    "                        bounding_box_info = f\"in the bounding box: {bbox}, \"\n",
    "\n",
    "                    # Prompting for prediction of the next 2 bounding boxes\n",
    "                    next_bounding_box_coordinates = []\n",
    "                    for i in range(1, 3):\n",
    "                        next_frame_index = frame_index + i * 10\n",
    "                        if next_frame_index < len(pedestrian_data['frames']):\n",
    "                            next_bbox = pedestrian_data['bbox'][next_frame_index]\n",
    "                            next_bounding_box_coordinates.append(next_bbox)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    next_bounding_box_info = \"\"\n",
    "                    if next_bounding_box_coordinates:\n",
    "                        next_bounding_box_info = \"The predicted trajectory for the pedestrian for the next 1 second is: \"\n",
    "                        next_bounding_box_info += ', '.join([f\"[{bbox}]\" for bbox in next_bounding_box_coordinates if bbox])\n",
    "                        next_bounding_box_info += \" respectively.\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    prompt = {\n",
    "                        \"Video ID\": video_id,\n",
    "                        \"Ped_id\": pedestrian_id,\n",
    "                        \"Frame Number\": frame_num,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\" Picture: {os.path.join(output_pedestrian_dir, f'Pedestrian_{pedestrian_id}_Image_{frame_num}.png')}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {pedestrian_id} in {video_id}?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Of course. The pedestrian identified as '{pedestrian_id}' is present in the bounding box: {pedestrian_data['bbox'][frame_index]}, is of the {age} age group, and its gender is {gender}. Its motion direction is {motion_direction}, and it is currently {action}. The pedestrian is {cross} the road, while its reaction is {reaction}. Also, The pedestrian maintains a {hand_gesture} hand gesture throughout the video. Moreover, the pedestrian is {look} at the vehicle and {nod}.\" \n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you tell me about the vehicle movement?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"For the vehicle, it is '{vehicle}'\" \n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you describe the entire traffic scene?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'.\" \n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                            },\n",
    "                             {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"{next_bounding_box_info}\"\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                    \n",
    "                    #print(\"Prompt:\")\n",
    "                    #print(json.dumps(prompt, indent=4))\n",
    "                    \n",
    "                if frames_processed >= max_frames_per_pedestrian:\n",
    "                    break\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "with open('output_prompts_Kiran(Type2).json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_Kiran(Type2).json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d508660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "images_dir = \"./images\"\n",
    "\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 1\n",
    "videos_processed = 0\n",
    "\n",
    "max_frames_per_pedestrian = 5\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "all_video_prompts = []\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    #print(output_pedestrian_dir)\n",
    "    #os.makedirs(output_video_dir, exist_ok=True)\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    #print(vehicle_annotations.keys())\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    #print(traffic_annotations.keys())\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    #print(ped_annotations.keys())\n",
    "        \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "    '''\n",
    "    print(\"Keys and Values in first_frame_traffic_attributes:\")\n",
    "    for key, value in first_frame_traffic_attributes.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    '''\n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        \n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "\n",
    "        middle_frame_index = len(pedestrian_data['frames']) // 2\n",
    "        middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "        frames_processed = 0\n",
    "\n",
    "        first_frame_number = pedestrian_data['frames'][0]\n",
    "\n",
    "        frames_to_process = pedestrian_data['frames'][::10][:max_frames_per_pedestrian]\n",
    "        \n",
    "        for frame_num in frames_to_process:\n",
    "            if frame_num <= num_frames:\n",
    "                frames_processed += 1\n",
    "\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "\n",
    "                    bounding_box_info = \"\"\n",
    "                    if frame_num in pedestrian_data['bbox']:\n",
    "                        bbox = pedestrian_data['bbox'][frame_num]\n",
    "                        bounding_box_info = f\"in the bounding box: {bbox}, \"\n",
    "\n",
    "                    # Prompting for prediction of the next 2 bounding boxes\n",
    "                    next_bounding_box_coordinates = []\n",
    "                    for i in range(1, 3):\n",
    "                        next_frame_index = frame_index + i * 10\n",
    "                        if next_frame_index < len(pedestrian_data['frames']):\n",
    "                            next_bbox = pedestrian_data['bbox'][next_frame_index]\n",
    "                            next_bounding_box_coordinates.append(next_bbox)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    next_bounding_box_info = \"\"\n",
    "                    if next_bounding_box_coordinates:\n",
    "                        next_bounding_box_info = \"The predicted trajectory for the pedestrian for the next 1 second is: \"\n",
    "                        next_bounding_box_info += ', '.join([f\"[{bbox}]\" for bbox in next_bounding_box_coordinates if bbox])\n",
    "                        next_bounding_box_info += \" respectively.\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    prompt = {\n",
    "                        \"Video ID\": video_id,\n",
    "                        \"Ped_id\": pedestrian_id,\n",
    "                        \"Frame Number\": frame_num,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Picture: {os.path.join(output_pedestrian_dir, f'Pedestrian_{pedestrian_id}_Image_{frame_num}.png')}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {pedestrian_data['bbox'][frame_index]}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian currently engaged in crossing the road?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian is {cross} the road.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"What is the pedestrian's motion direction?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's motion direction is {motion_direction}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Can you describe the pedestrian's action?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian is currently {action}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"What is the pedestrian's reaction?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's reaction is {reaction}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian making any specific hand gestures?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian maintains a {hand_gesture} hand gesture throughout the video.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian looking at the vehicle?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, the pedestrian is {look} at the vehicle.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian nodding?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, the pedestrian is {nod}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you tell me about the vehicle movement?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"For the vehicle, it is '{vehicle}'.\" \n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you describe the entire traffic scene?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'.\" \n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"{next_bounding_box_info}\"\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                    \n",
    "\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                    \n",
    "                    print(\"Prompt:\")\n",
    "                    print(json.dumps(prompt, indent=4))\n",
    "                    \n",
    "                if frames_processed >= max_frames_per_pedestrian:\n",
    "                    break\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "with open('output_prompts_Kiran(Type3).json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_Kiran(Type3).json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "images_dir = \"./images\"\n",
    "\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 1\n",
    "videos_processed = 0\n",
    "\n",
    "max_frames_per_pedestrian = 5\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "all_video_prompts = []\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    #print(output_pedestrian_dir)\n",
    "    #os.makedirs(output_video_dir, exist_ok=True)\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    #print(vehicle_annotations.keys())\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    #print(traffic_annotations.keys())\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    #print(ped_annotations.keys())\n",
    "        \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "    '''\n",
    "    print(\"Keys and Values in first_frame_traffic_attributes:\")\n",
    "    for key, value in first_frame_traffic_attributes.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    '''\n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        \n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "\n",
    "        middle_frame_index = len(pedestrian_data['frames']) // 2\n",
    "        middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "        frames_processed = 0\n",
    "\n",
    "        first_frame_number = pedestrian_data['frames'][0]\n",
    "\n",
    "        frames_to_process = pedestrian_data['frames'][::10][:max_frames_per_pedestrian]\n",
    "        \n",
    "        for frame_num in frames_to_process:\n",
    "            if frame_num <= num_frames:\n",
    "                frames_processed += 1\n",
    "\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "\n",
    "                    bounding_box_info = \"\"\n",
    "                    if frame_num in pedestrian_data['bbox']:\n",
    "                        bbox = pedestrian_data['bbox'][frame_num]\n",
    "                        bounding_box_info = f\"in the bounding box: {bbox}, \"\n",
    "\n",
    "                    next_bounding_box_coordinates = []\n",
    "                    for i in range(1, 3):\n",
    "                        next_frame_index = frame_index + i * 10\n",
    "                        if next_frame_index < len(pedestrian_data['frames']):\n",
    "                            next_bbox = pedestrian_data['bbox'][next_frame_index]\n",
    "                            next_bounding_box_coordinates.append(next_bbox)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    next_bounding_box_info = \"\"\n",
    "                    if next_bounding_box_coordinates:\n",
    "                        next_bounding_box_info = \"The predicted trajectory for the pedestrian for the next 1 second is: \"\n",
    "                        next_bounding_box_info += ', '.join([f\"[{bbox}]\" for bbox in next_bounding_box_coordinates if bbox])\n",
    "                        next_bounding_box_info += \" respectively.\"\n",
    "                    \n",
    "                    \n",
    "                    prompt = {\n",
    "                        \"Video ID\": video_id,\n",
    "                        \"Ped_id\": pedestrian_id,\n",
    "                        \"Frame Number\": frame_num,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\" Picture: {os.path.join(output_pedestrian_dir, f'Pedestrian_{pedestrian_id}_Image_{frame_num}.png')}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {pedestrian_id} in {video_id}, as well as the vehicle movement and the entire traffic scene?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Of course. The pedestrian identified as '{pedestrian_id}' is present in the bounding box: {pedestrian_data['bbox'][frame_index]}, is of the {age} age group, and its gender is {gender}. Its motion direction is {motion_direction}, and it is currently {action}. The pedestrian is {cross} the road, while its reaction is {reaction}. Also, The pedestrian maintains a {hand_gesture} hand gesture throughout the video. Moreover, the pedestrian is {look} at the vehicle and {nod}. For the vehicle, it is '{vehicle}'. Moreover, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                            },\n",
    "                             {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"{next_bounding_box_info}\"\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                    \n",
    "                    print(\"Prompt:\")\n",
    "                    print(json.dumps(prompt, indent=4))\n",
    "                    \n",
    "                if frames_processed >= max_frames_per_pedestrian:\n",
    "                    break\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "with open('output_prompts_Kiran(Type1).json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_Kiran(Type1).json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f81ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "images_dir = \"./images\"\n",
    "\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 1\n",
    "videos_processed = 0\n",
    "\n",
    "max_frames_per_pedestrian = 5\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "all_video_prompts = []\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    #print(output_pedestrian_dir)\n",
    "    #os.makedirs(output_video_dir, exist_ok=True)\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    #print(vehicle_annotations.keys())\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    #print(traffic_annotations.keys())\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    #print(ped_annotations.keys())\n",
    "        \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "    '''\n",
    "    print(\"Keys and Values in first_frame_traffic_attributes:\")\n",
    "    for key, value in first_frame_traffic_attributes.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    '''\n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        \n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "\n",
    "        middle_frame_index = len(pedestrian_data['frames']) // 2\n",
    "        middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "        frames_processed = 0\n",
    "\n",
    "        first_frame_number = pedestrian_data['frames'][0]\n",
    "\n",
    "        frames_to_process = pedestrian_data['frames'][::10][:max_frames_per_pedestrian]\n",
    "        \n",
    "        for frame_num in frames_to_process:\n",
    "            if frame_num <= num_frames:\n",
    "                frames_processed += 1\n",
    "\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "\n",
    "                    bounding_box_info = \"\"\n",
    "                    if frame_num in pedestrian_data['bbox']:\n",
    "                        bbox = pedestrian_data['bbox'][frame_num]\n",
    "                        bounding_box_info = f\"in the bounding box: {bbox}, \"\n",
    "\n",
    "                    # Prompting for prediction of the next 2 bounding boxes\n",
    "                    next_bounding_box_coordinates = []\n",
    "                    for i in range(1, 3):\n",
    "                        next_frame_index = frame_index + i * 10\n",
    "                        if next_frame_index < len(pedestrian_data['frames']):\n",
    "                            next_bbox = pedestrian_data['bbox'][next_frame_index]\n",
    "                            next_bounding_box_coordinates.append(next_bbox)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    next_bounding_box_info = \"\"\n",
    "                    if next_bounding_box_coordinates:\n",
    "                        next_bounding_box_info = \"The predicted trajectory for the pedestrian for the next 1 second is: \"\n",
    "                        next_bounding_box_info += ', '.join([f\"[{bbox}]\" for bbox in next_bounding_box_coordinates if bbox])\n",
    "                        next_bounding_box_info += \" respectively.\"\n",
    "\n",
    "                    occlusion_sentence = (\n",
    "                        \"The pedestrian in this video is fully visible, without any obstructions.\" if occlusion_mapping == 0 else\n",
    "                        \"The pedestrian in this video is partially visible, there's some obstruction in the view.\" if occlusion_mapping == 1 else\n",
    "                        \"The pedestrian is fully obstructed in the video angle and cannot be seen clearly.\"\n",
    "                    )\n",
    "\n",
    "                    action_sentence = (\n",
    "                        \"The pedestrian is currently standing still.\" if action == 0 else\n",
    "                        \"The pedestrian is currently walking.\"\n",
    "                    )\n",
    "\n",
    "                    nod_sentence = (\n",
    "                        \"The pedestrian is nodding in agreement.\" if nod == 1 else\n",
    "                        \"The pedestrian is not nodding.\"\n",
    "                    )\n",
    "\n",
    "                    look_sentence = (\n",
    "                        \"The pedestrian is actively looking around.\" if look == 1 else\n",
    "                        \"The pedestrian is not actively looking around.\"\n",
    "                    )\n",
    "\n",
    "                    hand_gesture_sentence = (\n",
    "                        f\"The pedestrian is making a {hand_gesture} hand gesture.\" if hand_gesture != 0 else\n",
    "                        \"The pedestrian's hand gesture is not identifiable.\"\n",
    "                    )\n",
    "\n",
    "                    reaction_sentence = (\n",
    "                        f\"The pedestrian's reaction suggests a clear path ahead.\" if reaction == 1 else\n",
    "                        f\"The pedestrian's reaction suggests they are speeding up.\" if reaction == 2 else\n",
    "                        f\"The pedestrian's reaction suggests they are slowing down.\" if reaction == 3 else\n",
    "                        \"The pedestrian's reaction is unclear.\"\n",
    "                    )\n",
    "\n",
    "                    cross_sentence = (\n",
    "                        \"The pedestrian is currently crossing the road.\" if cross == 1 else\n",
    "                        \"The pedestrian is not currently crossing the road.\" if cross == 0 else\n",
    "                        \"The status of pedestrian crossing is irrelevant.\"\n",
    "                    )\n",
    "\n",
    "                    # Generate the combined paragraph for the prompt\n",
    "                    prompt_paragraph = (\n",
    "                        f\"{occlusion_sentence} {action_sentence} {nod_sentence} \"\n",
    "                        f\"{look_sentence} {hand_gesture_sentence} {reaction_sentence} {cross_sentence}\"\n",
    "                    )\n",
    "\n",
    "                    \n",
    "                    prompt = {\n",
    "                        \"conversations\": [\n",
    "                        {\n",
    "                            \"from\": \"user\",\n",
    "                            \"value\": (\n",
    "                                \"Context: Your job is to predict the pedestrian trajectory of pedestrians. \"\n",
    "                                \"I will give you images that are frames from a video and your role is to predict what the pedestrian will do next. \"\n",
    "                                f\"Here is some information based on the pedestrian {pedestrian_id} in {video_id}:\"\n",
    "                                f\"Input: {pedestrian_id} who is bounded by the box  ,Age: adult\\nGender: female\\nDirection of motion: LONGITUDE\\nCurrent action: walking\\nType of road: parking_lot\\nPedestrian crossing: 'Absent'\\nPedestrian sign: 'Absent'\\nState of traffic light: n/a\\nVehicle: moving_slow\\n\\nThe pedestrian is not-crossing the road, while its reaction is undefined. Also, The pedestrian maintains a undefined hand gesture throughout. The pedestrian is not-looking. The pedestrian nodding state is not-nodding.\\n\\n\"\n",
    "                                \"Role:\\nNow, based on the images, your role is to predict what the pedestrian will do next in the next 1 second. Analyze the pedestrian behaviour and predict their trajectory.\"\n",
    "                            )\n",
    "                        },\n",
    "                            {\n",
    "                                \"from\": \"GPT\",\n",
    "                                \"value\": \"The pedestrian will cross the road after 1 second because the combination of their raised hand gesture, indicating a desire to halt any potentially oncoming traffic, the current green traffic light for pedestrians, the absence of vehicles in the immediate vicinity, and their consistent gaze towards the opposite sidewalk suggests they are actively assessing the safety of crossing at this moment. Given these conditions, including their positioning at the edge of the pedestrian crossing zone, it's highly probable they will initiate crossing the road within the next second.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                    \n",
    "                    print(\"Prompt:\")\n",
    "                    print(json.dumps(prompt, indent=4))\n",
    "                    \n",
    "                if frames_processed >= max_frames_per_pedestrian:\n",
    "                    break\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "with open('output_prompts_Kiran(Type4).json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_Kiran(Type4).json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc473d9",
   "metadata": {},
   "source": [
    "This code creates prompt for every frame, contains information of 5 previous bboxes, current bbox, and upcoming 5 bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "images_dir = \"./images\"\n",
    "output_main_dir = \"./images_with_boxes(Pedestrians Focused)\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 1\n",
    "videos_processed = 0\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "all_video_prompts = []\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "\n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "\n",
    "        for frame_num in pedestrian_data['frames']:\n",
    "            if frame_num <= num_frames:\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    middle_frame_attributes = pedestrian_data['attributes']\n",
    "\n",
    "                    age = age_mapping.get(middle_frame_attributes.get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(middle_frame_attributes.get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(middle_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "                    # Get current bounding box coordinates\n",
    "                    current_bbox = pedestrian_data['bbox'][frame_num]\n",
    "\n",
    "                    # Get previous 5 bounding boxes\n",
    "                    previous_bboxes = []\n",
    "                    for i in range(frame_index - 5, frame_index):\n",
    "                        if i >= 0:\n",
    "                            previous_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "\n",
    "                    # Get future 5 bounding boxes\n",
    "                    future_bboxes = []\n",
    "                    for i in range(frame_index + 1, min(frame_index + 6, len(pedestrian_data['frames']))):\n",
    "                        future_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "\n",
    "                    # Construct the prompt\n",
    "                    prompt = {\n",
    "                        \"Video ID\": video_id,\n",
    "                        \"Ped_id\": pedestrian_id,\n",
    "                        \"Frame Number\": frame_num,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Picture: {os.path.join(output_pedestrian_dir, f'Pedestrian_{pedestrian_id}_Image_{frame_num}.png')}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {current_bbox}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The 5 bounding boxes for previous frames are: {previous_bboxes}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian currently engaged in crossing the road?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian is {cross} the road.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"What is the pedestrian's motion direction?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's motion direction is {motion_direction}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Can you describe the pedestrian's action?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian is currently {action}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"What is the pedestrian's reaction?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's reaction is {reaction}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian making any specific hand gestures?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian maintains a {hand_gesture} hand gesture throughout the video.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian looking at the vehicle?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, the pedestrian is {look} at the vehicle.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian nodding?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, the pedestrian is {nod}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you tell me about the vehicle movement?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"For the vehicle, it is '{vehicle}'.\" \n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you describe the entire traffic scene?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'.\" \n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The future 5 bounding boxes are: {future_bboxes}\"\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "\n",
    "                    #print(\"Prompt:\")\n",
    "                    #print(json.dumps(prompt, indent=4))\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "with open('output_prompts_Kiran(Type5)_updated(22Apr).json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_Kiran(Type5)_updated(22Apr).json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "images_dir = \"./images\"\n",
    "output_main_dir = \"./Prompts_for_Pedestrians\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 346\n",
    "videos_processed = 0\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "\n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "    \n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in ped_annotations.items():\n",
    "        output_pedestrian_dir = output_video_dir  # Remove subfolder creation\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "        \n",
    "        all_video_prompts = []\n",
    "\n",
    "        for frame_num in pedestrian_data['frames']:\n",
    "            if frame_num <= num_frames:\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    #print(f\"Processing frame {frame_num} for pedestrian {pedestrian_id}\")\n",
    "\n",
    "                    age = age_mapping.get(pedestrian_data['attributes'].get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(pedestrian_data['attributes'].get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(pedestrian_data['attributes'].get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "                    if frame_num < len(pedestrian_data['bbox']):\n",
    "                        current_bbox = pedestrian_data['bbox'][frame_num]\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                 \n",
    "                    previous_bboxes = []\n",
    "                    for i in range(frame_index - 1, max(frame_index - 6, -1), -1):\n",
    "                        if pedestrian_data['frames'][i] < len(pedestrian_data['bbox']):\n",
    "                            previous_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "                    #else:\n",
    "                        #print(f\"Warning: Bbox index out of range for frame {pedestrian_data['frames'][i]}\")\n",
    "\n",
    "\n",
    "                    future_bboxes = []\n",
    "                    for i in range(frame_index + 1, min(frame_index + 6, len(pedestrian_data['frames']))):\n",
    "                        if i < len(pedestrian_data['frames']) and pedestrian_data['frames'][i] < len(pedestrian_data['bbox']):\n",
    "                            future_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "                        else:\n",
    "                            #print(f\"Warning: Bbox index out of range for frame {pedestrian_data['frames'][i]}\")\n",
    "                            break  \n",
    "                    \n",
    "                    #print(f\"Creating prompt for pedestrian {pedestrian_id}, frame {frame_num}\")\n",
    "\n",
    "                    prompt = {\n",
    "                        \"Video ID\": video_id,\n",
    "                        \"Ped_id\": pedestrian_id,\n",
    "                        \"Frame Number\": frame_num,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Picture: ./images_with_boxes(Pedestrians Focused)/{video_id}/Pedestrian_{pedestrian_id}/Pedestrian_{pedestrian_id}_Image_{frame_num}.png\"\n",
    "\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {pedestrian_data['bbox'][frame_index]}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The 5 bounding boxes for previous frames are: {previous_bboxes}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian currently engaged in crossing the road?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian is {cross} the road.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"What is the pedestrian's motion direction?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's motion direction is {motion_direction}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Can you describe the pedestrian's action?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian is currently {action}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"What is the pedestrian's reaction?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's reaction is {reaction}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian making any specific hand gestures?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian maintains a {hand_gesture} hand gesture throughout the video.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian looking at the vehicle?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, the pedestrian is {look} at the vehicle.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Is the pedestrian nodding?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, the pedestrian is {nod}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you tell me about the vehicle movement?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"For the vehicle, it is '{vehicle}'.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Great! Now can you describe the entire traffic scene?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"Yes, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": \"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The future 5 bounding boxes are: {future_bboxes}\"\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                #else:\n",
    "                    #print(f\"No behavior data found for frame {frame_num} of pedestrian {pedestrian_id}\")\n",
    "\n",
    "                    #print(\"Prompt:\")\n",
    "                    #print(json.dumps(prompt, indent=4))\n",
    "        if all_video_prompts:\n",
    "\n",
    "            with open(os.path.join(output_video_dir, f'pedestrian_{pedestrian_id}_prompts.json'), 'w') as f:\n",
    "                json.dump(all_video_prompts, f, indent=4)\n",
    "        #else:\n",
    "            #print(f\"No prompts generated for pedestrian {pedestrian_id}\")\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "print(\"Prompts generated and saved to respective folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0cce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "images_dir = \"./images\"\n",
    "output_main_dir = \"./Prompts_for_Pedestrians\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 346\n",
    "videos_processed = 0\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in ped_annotations.items():\n",
    "        output_pedestrian_dir = output_video_dir  # Remove subfolder creation\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "\n",
    "        simplified_prompts = []\n",
    "\n",
    "        # Check if pedestrian has behavior data\n",
    "        if 'behavior' not in pedestrian_data or not pedestrian_data['behavior']:\n",
    "            print(f\"No behavior data found for pedestrian {pedestrian_id}. Creating prompts based on bounding boxes only.\")\n",
    "\n",
    "            for frame_num in pedestrian_data['frames']:\n",
    "                if frame_num <= video_data['num_frames']:\n",
    "                    frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                    # Get current bounding box coordinates\n",
    "                    if frame_index < len(pedestrian_data['bbox']):\n",
    "                        current_bbox = pedestrian_data['bbox'][frame_index]\n",
    "                    else:\n",
    "                        continue  # Skip this frame if bbox index is out of range\n",
    "\n",
    "                    # Get previous 5 bounding boxes\n",
    "                    previous_bboxes = []\n",
    "                    for i in range(max(frame_index - 5, 0), frame_index):\n",
    "                        if i < len(pedestrian_data['bbox']):\n",
    "                            previous_bboxes.append(pedestrian_data['bbox'][i])\n",
    "\n",
    "                    # Get future 5 bounding boxes\n",
    "                    future_bboxes = []\n",
    "                    for i in range(frame_index + 1, min(frame_index + 6, len(pedestrian_data['bbox']))):\n",
    "                        future_bboxes.append(pedestrian_data['bbox'][i])\n",
    "\n",
    "                    # Construct simplified prompt\n",
    "                    simplified_prompt = {\n",
    "                        \"Video ID\": video_id,\n",
    "                        \"Ped_id\": pedestrian_id,\n",
    "                        \"Frame Number\": frame_num,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Picture: ./images_with_boxes(Pedestrians Focused)/{video_id}/Pedestrian_{pedestrian_id}/Pedestrian_{pedestrian_id}_Image_{frame_num}.png\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Role: You are tasked with enhancing the pedestrian detection and its immediate trajectory prediction system for an autonomous vehicle. Firstly, can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {current_bbox}.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The 5 bounding boxes for previous frames are: {previous_bboxes}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"Thank you for the information. Can you also assist me in predicting the trajectories of pedestrians in the next 0.5 and 1 seconds?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The future 5 bounding boxes are: {future_bboxes}\"\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    simplified_prompts.append(simplified_prompt)  # Append the simplified prompt to the list\n",
    "\n",
    "            # Save simplified prompts\n",
    "            with open(os.path.join(output_pedestrian_dir, f'pedestrian_{pedestrian_id}_prompts.json'), 'w') as f:\n",
    "                json.dump(simplified_prompts, f, indent=4)\n",
    "\n",
    "            videos_processed += 1\n",
    "\n",
    "print(\"Simplified prompts generated and saved to respective folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "images_dir = \"./images\"\n",
    "output_main_dir = \"./Prompts_for_Pedestrians\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 346\n",
    "videos_processed = 0\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "\n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    \n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "\n",
    "    num_frames = video_data['num_frames']\n",
    "    \n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in ped_annotations.items():\n",
    "        output_pedestrian_dir = output_video_dir  # Remove subfolder creation\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "        \n",
    "        all_video_prompts = []\n",
    "\n",
    "        for frame_num in pedestrian_data['frames']:\n",
    "            if frame_num <= num_frames:\n",
    "                frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "                    #print(f\"Processing frame {frame_num} for pedestrian {pedestrian_id}\")\n",
    "\n",
    "                    age = age_mapping.get(pedestrian_data['attributes'].get('age', 0), 'Unknown')\n",
    "                    gender = gender_mapping.get(pedestrian_data['attributes'].get('gender', 0), 'Unknown')\n",
    "                    motion_direction = motion_direction_mapping.get(pedestrian_data['attributes'].get('motion_direction', 0), 'Unknown')\n",
    "                    action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "                    cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "                    reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "                    hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "                    look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "                    nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "                    vehicle = vehicle_mapping.get(vehicle_annotations[frame_index], 'Unknown')\n",
    "\n",
    "                    if frame_num < len(pedestrian_data['bbox']):\n",
    "                        current_bbox = pedestrian_data['bbox'][frame_num]\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                 \n",
    "                    previous_bboxes = []\n",
    "                    for i in range(frame_index - 1, max(frame_index - 6, -1), -1):\n",
    "                        if pedestrian_data['frames'][i] < len(pedestrian_data['bbox']):\n",
    "                            previous_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "                    #else:\n",
    "                        #print(f\"Warning: Bbox index out of range for frame {pedestrian_data['frames'][i]}\")\n",
    "\n",
    "\n",
    "                    future_bboxes = []\n",
    "                    for i in range(frame_index + 1, min(frame_index + 6, len(pedestrian_data['frames']))):\n",
    "                        if i < len(pedestrian_data['frames']) and pedestrian_data['frames'][i] < len(pedestrian_data['bbox']):\n",
    "                            future_bboxes.append(pedestrian_data['bbox'][pedestrian_data['frames'][i]])\n",
    "                        else:\n",
    "                            #print(f\"Warning: Bbox index out of range for frame {pedestrian_data['frames'][i]}\")\n",
    "                            break  \n",
    "                    \n",
    "                    #print(f\"Creating prompt for pedestrian {pedestrian_id}, frame {frame_num}\")\n",
    "\n",
    "                    prompt = {\n",
    "                        \"id\": pedestrian_id,\n",
    "                        \"image\": f\"./images_with_boxes_Pedestrians Focused_/{video_id}/Pedestrian_{pedestrian_id}/Pedestrian_{pedestrian_id}_Image_{frame_num}.png\",\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\"<image> Can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present and the past 5 frames? Is the pedestrian currently engaged in crossing the road? What is the pedestrian's motion direction? Can you tell the pedestrian's action? What is the pedestrian's reaction? Is the pedestrian making any specific hand gestures? Is the pedestrian looking at the vehicle? Is the pedestrian nodding? Also, can you tell me about the vehicle movement? Moreover can you describe the entire traffic scene? Can you also assist me in predicting the trajectories of pedestrians in the next 5 frames?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {pedestrian_data['bbox'][frame_index]}. The 5 bounding boxes for previous frames are: {previous_bboxes}. The pedestrian is {cross} the road. The pedestrian's motion direction is {motion_direction}. The pedestrian is currently {action}. The pedestrian's reaction is {reaction}. The pedestrian maintains a {hand_gesture} hand gesture throughout the video. The pedestrian is {look} at the vehicle and the pedestrian is {nod}. For the vehicle attributes, it is '{vehicle}'. While, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'. The future 5 bounding boxes are: {future_bboxes}.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    all_video_prompts.append(prompt)\n",
    "                #else:\n",
    "                    #print(f\"No behavior data found for frame {frame_num} of pedestrian {pedestrian_id}\")\n",
    "\n",
    "                    #print(\"Prompt:\")\n",
    "                    #print(json.dumps(prompt, indent=4))\n",
    "        if all_video_prompts:\n",
    "\n",
    "            with open(os.path.join(output_video_dir, f'pedestrian_{pedestrian_id}_prompts.json'), 'w') as f:\n",
    "                json.dump(all_video_prompts, f, indent=4)\n",
    "        #else:\n",
    "            #print(f\"No prompts generated for pedestrian {pedestrian_id}\")\n",
    "\n",
    "    videos_processed += 1\n",
    "\n",
    "print(\"Prompts generated and saved to respective folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "images_dir = \"./images\"\n",
    "output_main_dir = \"./Prompts_for_Pedestrians\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "num_videos_to_process = 346\n",
    "videos_processed = 0\n",
    "\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'not-nodding', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "for video_id, video_data in database.items():\n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    if videos_processed >= num_videos_to_process:\n",
    "        break\n",
    "\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "\n",
    "    for pedestrian_id, pedestrian_data in ped_annotations.items():\n",
    "        output_pedestrian_dir = output_video_dir  # Remove subfolder creation\n",
    "        os.makedirs(output_pedestrian_dir, exist_ok=True)\n",
    "\n",
    "        simplified_prompts = []\n",
    "\n",
    "        # Check if pedestrian has behavior data\n",
    "        if 'behavior' not in pedestrian_data or not pedestrian_data['behavior']:\n",
    "            print(f\"No behavior data found for pedestrian {pedestrian_id}. Creating prompts based on bounding boxes only.\")\n",
    "\n",
    "            for frame_num in pedestrian_data['frames']:\n",
    "                if frame_num <= video_data['num_frames']:\n",
    "                    frame_index = pedestrian_data['frames'].index(frame_num)\n",
    "\n",
    "                    # Get current bounding box coordinates\n",
    "                    if frame_index < len(pedestrian_data['bbox']):\n",
    "                        current_bbox = pedestrian_data['bbox'][frame_index]\n",
    "                    else:\n",
    "                        continue  # Skip this frame if bbox index is out of range\n",
    "\n",
    "                    # Get previous 5 bounding boxes\n",
    "                    previous_bboxes = []\n",
    "                    for i in range(max(frame_index - 5, 0), frame_index):\n",
    "                        if i < len(pedestrian_data['bbox']):\n",
    "                            previous_bboxes.append(pedestrian_data['bbox'][i])\n",
    "\n",
    "                    # Get future 5 bounding boxes\n",
    "                    future_bboxes = []\n",
    "                    for i in range(frame_index + 1, min(frame_index + 6, len(pedestrian_data['bbox']))):\n",
    "                        future_bboxes.append(pedestrian_data['bbox'][i])\n",
    "\n",
    "                    # Construct simplified prompt\n",
    "                    simplified_prompt = {\n",
    "                        \"id\": pedestrian_id,\n",
    "                        \"image\": f\"./images_with_boxes_Pedestrians Focused_/{video_id}/Pedestrian_{pedestrian_id}/Pedestrian_{pedestrian_id}_Image_{frame_num}.png\",\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"user\",\n",
    "                                \"value\": f\": <image> Can you provide insights on pedestrian {pedestrian_id} in {video_id}? Specifically, what is the coordinate of the pedestrian at present and for 5 previous frames?  Also, can you tell me about the vehicle movement? Moreover can you describe the entire traffic scene? Can you also assist me in predicting the trajectories of pedestrians in the next 5 frames?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"assistant\",\n",
    "                                \"value\": f\"The pedestrian's current location is within the bounding box: {current_bbox}. The 5 bounding boxes for previous frames are: {previous_bboxes}. For the vehicle attributes, it is '{vehicle}'. While, for the scene attributes, the road type is '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'. The future 5 bounding boxes are: {future_bboxes}.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    simplified_prompts.append(simplified_prompt)  # Append the simplified prompt to the list\n",
    "\n",
    "            # Save simplified prompts\n",
    "            with open(os.path.join(output_pedestrian_dir, f'pedestrian_{pedestrian_id}_prompts.json'), 'w') as f:\n",
    "                json.dump(simplified_prompts, f, indent=4)\n",
    "\n",
    "            videos_processed += 1\n",
    "\n",
    "print(\"Simplified prompts generated and saved to respective folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9a48b",
   "metadata": {},
   "source": [
    "For CogVLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "annotations_file = \"./data_cache/jaad_database.pkl\"\n",
    "\n",
    "with open(annotations_file, 'rb') as f:\n",
    "    database = pickle.load(f)\n",
    "\n",
    "# Value Mapping to words\n",
    "occlusion_mapping = {0: 'none', 1: 'part', 2: 'full'}\n",
    "action_mapping = {0: 'standing', 1: 'walking'}\n",
    "nod_mapping = {0: 'undefined', 1: 'nodding'}\n",
    "look_mapping = {0: 'not-looking', 1: 'looking'}\n",
    "hand_gesture_mapping = {0: 'undefined', 1: 'greet', 2: 'yield', 3: 'rightofway', 4: 'other'}\n",
    "reaction_mapping = {0: 'undefined', 1: 'clear_path', 2: 'speed_up', 3: 'slow_down'}\n",
    "cross_mapping = {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'}\n",
    "age_mapping = {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'}\n",
    "designated_mapping = {0: 'ND', 1: 'D'}\n",
    "gender_mapping = {0: 'n/a', 1: 'female', 2: 'male'}\n",
    "intersection_mapping = {0: 'no', 1: 'yes'}\n",
    "motion_direction_mapping = {0: 'n/a', 1: 'LATITUDE', 2: 'LONGITUDE'}\n",
    "traffic_direction_mapping = {0: 'OW', 1: 'TW'}\n",
    "signalized_mapping = {0: 'n/a', 1: 'NS', 2: 'S'}\n",
    "vehicle_mapping = {0: 'stopped', 1: 'moving_slow', 2: 'moving_fast', 3: 'decelerating', 4: 'accelerating'}\n",
    "road_type_mapping = {0: 'street', 1: 'parking_lot', 2: 'garage'}\n",
    "traffic_light_mapping = {0: 'n/a', 1: 'red', 2: 'green'}\n",
    "pedestrian_crossing_mapping = {0: 'Absent', 1: 'Present'}\n",
    "pedestrian_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "stop_sign_mapping = {0: 'Absent', 1: 'Present'}\n",
    "\n",
    "# Define the number of videos to process\n",
    "num_videos_to_process = 1\n",
    "\n",
    "all_video_prompts = []\n",
    "\n",
    "# Loop to iterate through each video\n",
    "for idx, (video_id, video_data) in enumerate(database.items()):\n",
    "    if idx >= num_videos_to_process:\n",
    "        break\n",
    "        \n",
    "    output_video_dir = os.path.join(output_main_dir, f\"{video_id}\")\n",
    "\n",
    "    #print(video_id)\n",
    "    #print(video_data.keys())\n",
    "    vehicle_annotations = video_data['vehicle_annotations']\n",
    "    #print(vehicle_annotations.keys())\n",
    "    traffic_annotations = video_data['traffic_annotations']\n",
    "    #print(traffic_annotations.keys())\n",
    "    ped_annotations = video_data['ped_annotations']\n",
    "    #print(ped_annotations.keys())\n",
    "        \n",
    "    first_frame_index = 0  \n",
    "    first_frame_traffic_attributes = traffic_annotations[first_frame_index]\n",
    "    '''\n",
    "    print(\"Keys and Values in first_frame_traffic_attributes:\")\n",
    "    for key, value in first_frame_traffic_attributes.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    '''\n",
    "    vehicle = vehicle_mapping.get(vehicle_annotations[first_frame_index], 'Unknown')\n",
    "    road_type = road_type_mapping.get(traffic_annotations.get('road_type', 0), 'Unknown')\n",
    "    \n",
    "    pedestrian_crossing = pedestrian_crossing_mapping.get(first_frame_traffic_attributes.get('ped_crossing', 0), 'Unknown')\n",
    "    pedestrian_sign = pedestrian_sign_mapping.get(first_frame_traffic_attributes.get('ped_sign', 0), 'Unknown')\n",
    "    stop_sign = stop_sign_mapping.get(first_frame_traffic_attributes.get('stop_sign', 0), 'Unknown')\n",
    "    traffic_light = traffic_light_mapping.get(first_frame_traffic_attributes.get('traffic_light', 0), 'Unknown')\n",
    "        \n",
    "    '''\n",
    "    print(\"Road Type:\", road_type)\n",
    "    print(\"Pedestrian Crossing:\", pedestrian_crossing)\n",
    "    print(\"Pedestrian Sign:\", pedestrian_sign)\n",
    "    print(\"Stop Sign:\", stop_sign)\n",
    "    print(\"Traffic Light:\", traffic_light)\n",
    "    '''\n",
    "\n",
    "    # Loop to iterate through each pedestrian in the video\n",
    "    for pedestrian_id, pedestrian_data in video_data['ped_annotations'].items():\n",
    "        \n",
    "        output_pedestrian_dir = os.path.join(output_video_dir, f\"Pedestrian_{pedestrian_id}\")\n",
    "\n",
    "        # Extract attributes for the first frame\n",
    "        first_frame_idx = pedestrian_data['frames'][0]\n",
    "        first_frame_attributes = pedestrian_data['attributes']\n",
    "        #print(first_frame_attributes)\n",
    "        first_frame_behavior = pedestrian_data['behavior']\n",
    "        #print(first_frame_behavior)\n",
    "        \n",
    "        if 'behavior' in pedestrian_data and 'action' in pedestrian_data['behavior'] and frame_index < len(pedestrian_data['behavior']['action']):\n",
    "            age = age_mapping.get(first_frame_attributes.get('age', 0), 'Unknown')\n",
    "            gender = gender_mapping.get(first_frame_attributes.get('gender', 0), 'Unknown')\n",
    "            motion_direction = motion_direction_mapping.get(first_frame_attributes.get('motion_direction', 0), 'Unknown')\n",
    "            action = action_mapping.get(pedestrian_data['behavior']['action'][frame_index], 'Unknown')\n",
    "            cross = cross_mapping.get(pedestrian_data['behavior'].get('cross', [])[frame_index], 'Unknown')\n",
    "            reaction = reaction_mapping.get(pedestrian_data['behavior'].get('reaction', [])[frame_index], 'Unknown')\n",
    "            hand_gesture = hand_gesture_mapping.get(pedestrian_data['behavior'].get('hand_gesture', [])[frame_index], 'Unknown')\n",
    "            look = look_mapping.get(pedestrian_data['behavior'].get('look', [])[frame_index], 'Unknown')\n",
    "            nod = nod_mapping.get(pedestrian_data['behavior'].get('nod', [])[frame_index], 'Unknown')\n",
    "\n",
    "\n",
    "        input_image_paths = []\n",
    "        frames_to_process = pedestrian_data['frames'][::10][:5]\n",
    "    \n",
    "        \n",
    "        for i, frame_num in enumerate(frames_to_process):\n",
    "            # Construct the path to the image\n",
    "            image_path = os.path.join(output_pedestrian_dir, f\"Pedestrian_{pedestrian_id}_Image_{frame_num}.png\")\n",
    "            input_image_paths.append(f\"image{i+1}: <img>{image_path}</img>\")\n",
    "\n",
    "        # Combine the input image paths into one line\n",
    "        input_images_line = ' '.join(input_image_paths)\n",
    "        \n",
    "        # Initialize the list to store bounding box coordinates\n",
    "        bounding_box_coordinates = []\n",
    "\n",
    "        # Bounding boxes for the first 5 pedestrian frames skipped by an interval of 10\n",
    "        for i in range(0, 50, 10):\n",
    "            # Check if the bounding box data exists at the current index\n",
    "            if i < len(pedestrian_data['bbox']):\n",
    "                bounding_box_data = pedestrian_data['bbox'][i]\n",
    "                bounding_box_coordinates.append(bounding_box_data)\n",
    "            else:\n",
    "                # Add a placeholder for missing bounding box data\n",
    "                bounding_box_coordinates.append([])\n",
    "\n",
    "        # Prompting for first 5 bounding boxes\n",
    "        bounding_box_info = \"\"\n",
    "        if any(bounding_box_coordinates):\n",
    "            bounding_box_info = \"The pedestrian of interest is marked by a green bounding box, the coordinates of the bounding boxes in each of the images above are \"\n",
    "            bounding_box_info += ', '.join([f\"[{bbox[0]}, {bbox[1]}, {bbox[2]}, {bbox[3]}]\" for bbox in bounding_box_coordinates if bbox])\n",
    "            bounding_box_info += \" respectively.\"\n",
    "\n",
    "        # Bounding boxes for the prediction part, containing the next 5 pedestrian frames skipped by an interval of 10\n",
    "        next_bounding_box_coordinates = []\n",
    "\n",
    "        for i in range(50, 100, 10):\n",
    "            if i < len(pedestrian_data['bbox']):\n",
    "                bounding_box_data = pedestrian_data['bbox'][i]\n",
    "                next_bounding_box_coordinates.append(bounding_box_data)\n",
    "            else:\n",
    "                next_bounding_box_coordinates.append([])\n",
    "\n",
    "        # Prompting for prediction of the next 5 bounding boxes\n",
    "        next_bounding_box_info = \"\"\n",
    "        if any(next_bounding_box_coordinates):\n",
    "            next_bounding_box_info = \"The predicted trajectory for the pedestrian for the next 1 second is: \"\n",
    "            next_bounding_box_info += ', '.join([f\"[{bbox[0]}, {bbox[1]}, {bbox[2]}, {bbox[3]}]\" for bbox in next_bounding_box_coordinates if bbox])\n",
    "            next_bounding_box_info += \" respectively.\"\n",
    "\n",
    "        # Final Prompt\n",
    "        prompt = {\n",
    "            \"Video ID\": video_id,\n",
    "            \"Ped_id\": pedestrian_id,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": f\"Role: You are an autonomous vehicle that uses front-camera images to interact with pedestrians. Input: {input_images_line}. Above are 5 sequential ego-vehicle front-camera view images extraced from a 2 second video that you can see behind the wheel.{bounding_box_info}\\nTask: predict the trajectory of the pedestrian of interest for the next 1 second.\\nExpected output: coordinates of 6 bounding box indicating the trajectory of pedestrain for the next 1 second(in the form of [((al1,bl1), (ar1,br1))],[((al6, b6), (ar6, br6))]).\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": f\"{next_bounding_box_info}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": \"According to the pictures and trajectory above, answer the following questions:\\n1. Describe the traffic situation (eg. environment, road condition, traffic sign, obstacles, other parties on the road) (optional)\\n2. Describe the behavior of the pedestrian of interest by considering the following aspects:\\n   - Is the pedestrian looking at the direction of the ego-vehicle?\\n   - Is the pedestrian crossing in front of the car?\\n3. Does the pedestrian interfere with ego-vehicle? What action should ego-vehicle take (stop, slow down or continue drive forward)\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": f\"The video is from a '{road_type}', the pedestrian crossing is '{pedestrian_crossing}', pedestrian sign is '{pedestrian_sign}', stop sign is '{stop_sign}', and the traffic light is '{traffic_light}'. The pedestrian is {cross} the road, while its also {look} at the car. For the vehicle, it is '{vehicle}'.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        all_video_prompts.append(prompt)\n",
    "        print(\"Prompt:\")\n",
    "        print(json.dumps(prompt, indent=4))\n",
    "        \n",
    "# Write prompts to a JSON file\n",
    "with open('output_prompts_liang.json', 'w') as f:\n",
    "    json.dump(all_video_prompts, f, indent=4)\n",
    "\n",
    "print(\"Prompts generated and saved to 'output_prompts_liang.json'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
